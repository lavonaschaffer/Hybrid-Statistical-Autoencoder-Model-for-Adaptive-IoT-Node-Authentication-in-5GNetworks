{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: One-Class Classifier Training for Anomaly Detection\n",
        "\n",
        "This notebook trains the one-class classifiers (OC-SVM, ECOD, EVT) using the trained autoencoder from Phase 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Enable GPU and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU is optional for Phase 2 (mostly sklearn/pyod models)\n",
        "# But still useful for autoencoder inference\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install numpy scikit-learn pyod scipy matplotlib seaborn joblib tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Upload Project Files via Google Drive (Recommended)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project structure\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "os.makedirs('ML_Project/src/models', exist_ok=True)\n",
        "os.makedirs('ML_Project/src/utils', exist_ok=True)\n",
        "os.makedirs('ML_Project/saved_models', exist_ok=True)\n",
        "os.makedirs('ML_Project/visualizations', exist_ok=True)\n",
        "os.makedirs('ML_Project/data/processed', exist_ok=True)\n",
        "\n",
        "print(\"Directory structure created.\")\n",
        "print(\"\\nPlease upload the following files to Google Drive, then update paths below:\")\n",
        "print(\"1. src/models/autoencoder.py\")\n",
        "print(\"2. src/models/classifier.py (if separate)\")\n",
        "print(\"3. src/train_classifier.py\")\n",
        "print(\"4. saved_models/ae_weights.pth (from Phase 1)\")\n",
        "print(\"5. data/processed/radioml_2018_processed.npz (optional, if using real data)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy files from Google Drive to project directory\n",
        "# Update these paths to match your Google Drive structure\n",
        "\n",
        "drive_base = '/content/drive/MyDrive'  # Adjust if your files are in a subfolder\n",
        "\n",
        "files_to_copy = {\n",
        "    'src/models/autoencoder.py': f'{drive_base}/autoencoder.py',\n",
        "    'src/train_classifier.py': f'{drive_base}/train_classifier.py',\n",
        "    'saved_models/ae_weights.pth': f'{drive_base}/ae_weights.pth',\n",
        "}\n",
        "\n",
        "print(\"Copying files from Google Drive...\")\n",
        "for target, source in files_to_copy.items():\n",
        "    target_path = f'ML_Project/{target}'\n",
        "    os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "    \n",
        "    if os.path.exists(source):\n",
        "        shutil.copy(source, target_path)\n",
        "        print(f\"✓ Copied {os.path.basename(source)}\")\n",
        "    else:\n",
        "        print(f\"✗ Not found: {source}\")\n",
        "        print(f\"  Please upload to Google Drive and update the path above\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Alternative: Upload Files Directly (for smaller files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you prefer direct upload (for smaller files only)\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"Upload the trained autoencoder weights (ae_weights.pth)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.pth'):\n",
        "        shutil.move(filename, 'ML_Project/saved_models/ae_weights.pth')\n",
        "        print(f\"✓ Moved {filename} to ML_Project/saved_models/\")\n",
        "    elif filename.endswith('.py'):\n",
        "        if 'autoencoder' in filename:\n",
        "            shutil.move(filename, 'ML_Project/src/models/autoencoder.py')\n",
        "        elif 'classifier' in filename or 'train' in filename:\n",
        "            shutil.move(filename, 'ML_Project/src/train_classifier.py')\n",
        "        print(f\"✓ Moved {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Verify Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check required files\n",
        "required_files = [\n",
        "    'ML_Project/src/models/autoencoder.py',\n",
        "    'ML_Project/src/train_classifier.py',\n",
        "    'ML_Project/saved_models/ae_weights.pth'\n",
        "]\n",
        "\n",
        "print(\"Checking required files...\")\n",
        "all_ok = True\n",
        "for file_path in required_files:\n",
        "    if os.path.exists(file_path):\n",
        "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "        print(f\"✓ {file_path} ({file_size:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"✗ {file_path} - MISSING!\")\n",
        "        all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\n✓ All files are ready!\")\n",
        "else:\n",
        "    print(\"\\n✗ Please upload the missing files!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train One-Class Classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change to project directory\n",
        "%cd ML_Project\n",
        "\n",
        "# Add src to Python path\n",
        "import sys\n",
        "sys.path.insert(0, 'src')\n",
        "\n",
        "# Import training components\n",
        "from train_classifier import OneClassTrainer, CONFIG\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Import autoencoder model\n",
        "try:\n",
        "    from models.autoencoder import RFAutoencoder\n",
        "    print(\"✓ Imported RFAutoencoder from models.autoencoder\")\n",
        "except ImportError:\n",
        "    # Fallback: use mock from train_classifier\n",
        "    from train_classifier import RFAutoencoder\n",
        "    print(\"✓ Using RFAutoencoder from train_classifier (mock)\")\n",
        "\n",
        "print(\"Setup complete. Starting Phase 2 training...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained autoencoder from Phase 1\n",
        "print(\"Loading trained autoencoder...\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize model - check which RFAutoencoder was imported\n",
        "try:\n",
        "    # Try Phase 1 architecture (from models.autoencoder)\n",
        "    model = RFAutoencoder(input_channels=2, seq_len=128, latent_dim=CONFIG['latent_dim']).to(device)\n",
        "    print(\"Using Phase 1 autoencoder architecture (Conv1D)\")\n",
        "    \n",
        "    # Load weights\n",
        "    try:\n",
        "        state_dict = torch.load('saved_models/ae_weights.pth', map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        model.eval()\n",
        "        print(\"✓ Autoencoder loaded successfully from Phase 1!\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Warning: Could not load autoencoder weights: {e}\")\n",
        "        print(\"Will use untrained model\")\n",
        "        \n",
        "except TypeError:\n",
        "    # Fallback to mock autoencoder (from train_classifier)\n",
        "    model = RFAutoencoder(input_len=256, latent_dim=CONFIG['latent_dim']).to(device)\n",
        "    print(\"Using mock autoencoder architecture (Linear layers)\")\n",
        "    \n",
        "    # Try to load weights anyway\n",
        "    try:\n",
        "        state_dict = torch.load('saved_models/ae_weights.pth', map_location=device)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        model.eval()\n",
        "        print(\"✓ Loaded compatible weights (some layers may not match)\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Using untrained mock autoencoder: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data based on which autoencoder we're using\n",
        "# Check if model expects (B, 2, 128) or (B, 256) input\n",
        "\n",
        "try:\n",
        "    # Try to load processed data\n",
        "    data = np.load('data/processed/radioml_2018_processed.npz')\n",
        "    X_train = torch.FloatTensor(data['X_train'])\n",
        "    print(f\"✓ Loaded processed data: {X_train.shape}\")\n",
        "    \n",
        "    # Check model architecture to determine input format\n",
        "    # Phase 1 model expects (N, 2, 128), mock expects (N, 256)\n",
        "    if hasattr(model, 'enc_conv1'):\n",
        "        # Phase 1 Conv1D model - keep as (N, 2, 128)\n",
        "        if len(X_train.shape) == 2:\n",
        "            # Reshape from (N, 256) to (N, 2, 128)\n",
        "            X_train = X_train.view(-1, 2, 128)\n",
        "        print(f\"Data shape for Conv1D model: {X_train.shape}\")\n",
        "    else:\n",
        "        # Mock Linear model - flatten to (N, 256)\n",
        "        if len(X_train.shape) == 3:\n",
        "            X_train = X_train.view(X_train.size(0), -1)\n",
        "        print(f\"Data shape for Linear model: {X_train.shape}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    # Generate synthetic data for demonstration\n",
        "    print(\"Processed data not found. Generating synthetic data...\")\n",
        "    samples = 5000\n",
        "    \n",
        "    if hasattr(model, 'enc_conv1'):\n",
        "        # Phase 1 model: (N, 2, 128)\n",
        "        X_train = torch.randn(samples, 2, 128).float()\n",
        "    else:\n",
        "        # Mock model: (N, 256)\n",
        "        X_train = torch.randn(samples, 256).float()\n",
        "    \n",
        "    print(f\"Generated synthetic data: {X_train.shape}\")\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(X_train)\n",
        "train_loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
        "print(f\"DataLoader created with batch_size={CONFIG['batch_size']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize trainer and train\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting One-Class Classifier Training\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "trainer = OneClassTrainer(model)\n",
        "trainer.train(train_loader)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training Complete!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained models\n",
        "print(\"\\nSaving models...\")\n",
        "trainer.save_state()\n",
        "print(\"✓ Models saved to saved_models/classifier_model.joblib\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Download Trained Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained classifier models\n",
        "from google.colab import files\n",
        "\n",
        "if os.path.exists('saved_models/classifier_model.joblib'):\n",
        "    files.download('saved_models/classifier_model.joblib')\n",
        "    print(\"✓ Classifier model downloaded!\")\n",
        "\n",
        "# Download visualizations if available\n",
        "if os.path.exists('visualizations/evt_distribution.png'):\n",
        "    files.download('visualizations/evt_distribution.png')\n",
        "    print(\"✓ Visualization downloaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Verify Model Loading (Optional Test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test loading the saved model\n",
        "from train_classifier import OneClassTrainer\n",
        "\n",
        "print(\"Testing model loading...\")\n",
        "try:\n",
        "    classifier_model = OneClassTrainer.load_state()\n",
        "    print(\"✓ Model loaded successfully!\")\n",
        "    print(f\"Model contains: {list(classifier_model.keys())}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading model: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips for Phase 2 Training\n",
        "\n",
        "1. **GPU Usage**: Phase 2 is less GPU-intensive than Phase 1, but GPU still helps with autoencoder inference\n",
        "2. **Training Time**: Classifier training is typically faster (minutes vs hours)\n",
        "3. **Models Saved**: All classifiers (OC-SVM, ECOD, EVT) are saved in one `classifier_model.joblib` file\n",
        "4. **Visualizations**: Check `visualizations/evt_distribution.png` for error distribution plot\n",
        "5. **Data**: Can use real processed data or synthetic data for testing\n",
        "6. **Memory**: Phase 2 uses less memory than Phase 1, so batch_size can be larger if needed\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
